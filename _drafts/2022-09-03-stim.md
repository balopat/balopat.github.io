---
title: Variable length shots with Stim
subtitle: A little trick sampling protocol to get the logical observable at any point during your measurement
layout: default
date: 2022-09-05
keywords: quantum computing, qec, css, cirq, stim
published: true
draft: true
---

# Background 

Stim is an excellent tool for sampling from stabilizer circuits. Stabilizer circuits are quantum algorithms that are made exclusively out of operators from the Clifford group, which is the normalizer of the Pauli group within the unitary group (meaning that conjugating Paulis with Cliffords results in Paulis, which means that you still get a Pauli even if you "commute it over a Clifford"). While this gate set is not universal, it is a great testing ground for stabilizer codes. 


## A serial QEC storage simulation workflow 
I found that reading most textbooks or papers, a serial kind of understanding of active quantum error correction schemes is the first that comes to mind. Our focus is storage simulation, i.e preserving quantum state in an error corrected memory. In a real quantum memory, after preparing a logical state, we run a loop of syndrome detection (sample), decoding a recovery operator (decode) and applying the recovery (either to a classical Pauli frame or to the device). In a simulation we are also interested whether our decoder was right. So, after a single sample, we decode, and we compare the predicted value of an observable (e.g. the value of logical $\bar{Z}$ for logical $|0\rangle$ state). A straightforward way to estimate the logical error rate is to repeat the `sample-decode-predict-compare` steps a lot of times, divide the number of errors by the total sample count. Here a _lot_ could be $10^8$ to $10^9$ times for a single data point in a threshold diagram to get a trustworthy estimate.  

It is quite natural to think of a simulation that follows in this serial spirit. From my small sample of QEC researchers, if they don't work with stim, they follow an algorithm that is sketched out here for a CSS code: 

**Algorithm 1.**

| Round    | Frame In               | Measurement            | Syndrome       | Detector                            | Frame out            | Notes                                                                        |
| -------- | ---------------------- | ---------------------- | -------------- | ----------------------------------- | -------------------- | ---------------------------------------------------------------------------- |
| $0$      | $0$                    | "Random" full syndrome | $s_0^+ = E_0H$ | $\sigma_0^+ = s_0^+$                | $E_0$                | this sets the random gauge, we'll land in one of the cosets of $\mathcal{C}$ |
| $1^+$    | $E_0$                  | Noisy full syndrome    | $s_1^+$        | $\sigma_1 = s_1 \oplus s_0^+$       | $E_0E_1$             |
| $\ldots$ | $\ldots$               | $\ldots$               | $\ldots$       | $\ldots$                            | $\ldots$             | $\ldots$                                                                     |
| $i^+$    | $\prod_{k=0}^{i-1}E_k$ | Noisy full syndrome    | $s_i^+$        | $\sigma_i = s_i^+ \oplus s_{i-1}^+$ | $\prod_{k=0}^{i}E_k$ | the $i$th noisy measurement round                                            |
        

1. prepare logical zero (can be skipped if we are doing a Pauli frame propagation only experiment) 
2. measure $s_i$ syndrome every round, under the given noise model, until the syndrome history is acceptable by the "decoder"
3. "decoder" determines recovery  $R^+$ based on the history
4. this should put us in the codespace + the internal errors of the $i$th round + errors coming from the fact that we might have fixed the wrong error, if it was only partially detected
5. we'd do a transversal $Z$ measurement (since our code is CSS) to the get the $X$-error frame 
6. we'd determine using classical decoding with $H_Z$ the ideal syndrome for the residual errors 
7. we'd get a final, ideal recovery operator $R^\circ$
8. we'd measure the parity of the frame - to determine the true value of the observable - if parity = 1 we have an error 

While this is a really flexible approach, and not too hard to implement a Pauli frame simulator that maintains the X and Z observable bits on each qubit, Craig Gidney uses a different approach in Stim, that enables an over 100-fold speedup the sampling part. While some of you might object that decoding still will be bottleneck, so why bother, I found several benefits to this approach.

## Offline Detector Decoding with Stim

I call the approach that is already visible in the [Stim getting started guide](https://github.com/quantumlib/Stim/blob/main/doc/getting_started.ipynb) "offline decoding", because instead of repeating `sample-decode-predict-compare` serially. we do `sample` first and `decode-predict-compare` offline on the generated samples. The key to the large speedup compared to a bare bones Pauli frame simulator is due to vectorization using 256 bit wide AVX instructions. When one samples a circuit, for example $10^6$ times, the $10^6$ shots are executed in batches of Pauli frames. Leveraging the AVX instructions, each quantum operation costs only a single operation, while in effect they change all the Pauli frames in the batch. While this is not entirely true for randomized operations like `DEPOL1` or `X_ERROR`, but Craig implemented speedups for those cases as well, using creative ways to sample from the binomial distribution. 


The algorithm 

The speedup per sample is a clear benefit, however, I see other 
- benefits:
  - speed
  - testability 
  - data sharing

## Limitations 
 - what if our rounds are of varying size? 
 - I spent months developing a direct QEC flow and then I sat down to work out this method 

# Key point


| Round     | Frame In               | Measurement                 | Syndrome                    | Detector                                     | Frame out            | Notes                                                                                                       |
| --------- | ---------------------- | --------------------------- | --------------------------- | -------------------------------------------- | -------------------- | ----------------------------------------------------------------------------------------------------------- |
| $0^+$     | $0$                    | "Random" full syndrome      | $s_0^+ = E_0H$              | $\sigma_0^+ = s_0^+$                         | $E_0$                | this sets the random gauge, we'll land in one of the cosets of $\mathcal{C}$                                |
| $0^+$     | $E_0$                  | Noiseless logical $\bar{Z}$ | $l_0^+ = E_0 \cdot \bar{Z}$ | $\lambda_0^+ = l_0^+$                        | $E_0$                | this also depends on the random gauge, can be 1 or 0                                                        |
| $0^\circ$ | $E_0$                  | Noiseless full syndrome     | $s_0^\circ$                 | $\sigma_0^\circ = s_0^+ \oplus s_0^\circ$    | $E_0$                | for sanity checking, but also for consistent round structure, $s_0^+ = s_0^\circ$, thus $\sigma_0^\circ =0$ |
| $0^\circ$ | $E_0$                  | Noiseless logical $\bar{Z}$ | $l_0^\circ$                 | $\lambda_0^\circ = l_0^+ \oplus l_0^\circ$   | $E_0$                | sanity check, $\lambda_0^+=0$ should hold                                                                   |
| $1^+$     | $E_0$                  | Noisy full syndrome         | $s_1^+$                     | $\sigma_1^+ = s_1 \oplus s_0^+$              | $E_0E_1$             | the first truly noisy measurement after the setup.                                                          |
| $1^+$     | $E_0E_1$               | Noiseless logical $\bar{Z}$ | $l_1^+$                     | $\lambda_1^+ = l_0^\circ \oplus l_1^+$       | $E_0E_1$             | captures the parity of $\bar{Z}$ against the full residual error of measuring $s_1^+$                       |
| $1^\circ$ | $E_0E_1$               | Noiseless full syndrome     | $s_1^\circ$                 | $\sigma_1^\circ = s_1^+ \oplus s_0$          | $E_0E_1$             | noiseless measurement for the ideal decoder in case we'd stop after 1 round                                 |
| $1^\circ$ | $E_0E_1$               | Noiseless logical $\bar{Z}$ | $l_1^\circ$                 | $\lambda_1^\circ = l_1^\circ \oplus l_1^+$   | $E_0E_1$             | sanity check & consistency $l_1^+=l_1^\circ$, thus, $\lambda_1^\circ=0$ always true                         |
| $\ldots$  | $\ldots$               | $\ldots$                    | $\ldots$                    | $\ldots$                                     | $\ldots$             | $\ldots$                                                                                                    |
| $i^+$     | $\prod_{k=0}^{i-1}E_k$ | Noisy full syndrome         | $s_i^+$                     | $\sigma_i^+ = s_i^+ \oplus s_{i-1}^+$        | $\prod_{k=0}^{i}E_k$ | the $i$th noisy measurement round                                                                           |
| $i^+$     | $\prod_{k=0}^{i}E_k$   | Noiseless logical $\bar{Z}$ | $l_i^+$                     | $\lambda_i^+ = l_{i-1}^\circ \oplus l_i^+$   | $\prod_{k=0}^{i}E_k$ | captures the parity of $\bar{Z}$ against the full residual error of measuring $s_1^+$                       |
| $i^\circ$ | $\prod_{k=0}^{i}E_k$   | Noiseless full syndrome     | $s_i^\circ$                 | $\sigma_i^\circ = s_i^+ \oplus s_{i-1}$      | $\prod_{k=0}^{i}E_k$ | noiseless measurement for the ideal decoder in case we'd stop after 1 round                                 |
| $i^\circ$ | $\prod_{k=0}^{i}E_k$   | Noiseless logical $\bar{Z}$ | $l_i^\circ$                 | $\lambda_i^\circ = l_i^\circ \oplus l_{i}^+$ | $\prod_{k=0}^{i}E_k$ | sanity check & consistency $l_i^+=l_i^\circ$, thus, $\lambda_i^\circ=0$ always true                         |


We can look at the 0th round as the one preparing the zero logical state. In reality it prepares the coset of the codespace and a random logical state. But, because we will be looking at the cumulated errors from this point forward, we can allow this to be our point of reference, i.e. we are not interested in the "true" syndrome in the next rounds, but in the syndrome _relative to our initial gauge_. 

In detector sampling mode, we only receive $\sigma_i$. However, it is easy to see, that any time we can reconstruct the $i$th syndrome by simply adding up the deltas, $s_i = \oplus_{k=0}^i \sigma_k$. 

The true observable (relative to the gauge) value will the sum of logicals. The decoder will predict the observed logical 

**Algorithm 2.**

1. state prep a random coset of the zero logical state (our gauge) by projecting with our syndrome measurements
2. measure syndromes and auxiliary data until the noisy syndrome history is acceptable (time decoder only sees $s_i^+$)
3. "space decoder" determines recovery  $R^+$ based on the history
4. as we know the true logical observable parity, we can update it with the parity of the recovery operator $R^+ \cdot \bar{Z}$
5. as $R^+H=s_i$ is guaranteed by a good decoder, we can emulate what syndrome an ideal decoder would measure after applying the $R^+$ by removing $s_i$ from the ideal syndrome $s_i^\circ$, i.e. we will use $\sigma_i^\circ$ to find $R^\circ$ with the space decoder
6. we check the parity of $R^\circ \cdot Z$
7. our final predicted observable parity is $R^\circ \cdot \bar{Z} \oplus R^+ \cdot \bar{Z} = R^+R^\circ \cdot \bar{Z}$
8. if predicted parity != true parity, then we have an error 
